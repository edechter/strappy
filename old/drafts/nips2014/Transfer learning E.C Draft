
Title: Transfer learning for inductive programming. 
Author: Josh Rule, Eyal Dechter, Josh Tenenbaum

Inductive programming algorithms attempt to induce computer programs from input/output examples of their desired behaviors. Previous work demonstrates that the performance of these algorithms improves in the multitask setting because it enables a learner to acquire a library of functions that concisely represent solutions to problems in a given domain. In these previous studies, however, different program induction tasks varied only in the target function to be learned, not in the type of function required; varying such properties of the target functions has been studied within the field of transfer learning, but has not yielded significant improvements in machine learning applications. Here, we show that inductive programing yields a natural and flexible approach for transfer learning by applying inductive programming to a transfer learning problem that every child faces and solves with some difficulty, that of learning the meanings of number concepts. Among other related tasks, young children are routinely asked to count the number of objects in a set, to produce N objects from a set, to compare the sizes of sets of objects and to combine such queries with other properties of objects (e.g. color or shape). We show that a multitask program induction approach to learning in this domain enables the learner to abstract out the common knowledge relating all these tasks from the task demands that are query specific. 
