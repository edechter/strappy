\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}


\begin{document}

\section{Bootstrapping Concept Learning by Iterated Search and Compression}

It is a commonly held intuition that an intelligent agent should be
able to extrapolate from multiple solutions of simple problems to
probable solutions of more complex ones in the same class. There have
been several attempts in AI and Machine Learning to put this intuition
into practice, but approaches have been generally limited to small
sets of heuristics (EBL) or restricted to vector-space representations
(transfer learning). The common idea, however, is to alter the
representation of an agent's hypothesis space in such a way that
solutions to new problems are more quickly found. Our goal here is to
propose a framework for such extrapolation based learning over
symbolic programs.

Our proposal is simple: a learning agent possesses some initial notion
of the complexity of various hypotheses in her hypothesis space.  When
presented with a set of problems of varying difficulty, she explores a
fixed volume of her hypothesis space in order of increasing
complexity. Having found a set of solutions to some of the problems in
that space, the learner chooses the most compressible set of these
solutions. The resulting compression induces a new complexity measure
on the hypothesis space. As the learner iterates this procedure, the
complexity landscape of the hypothesis space changes, allowing her to
reach solutions to additional problems while maintaining a fixed
search space size. 

The optimal compression for distribution over data will necessarily
assign shorter representations to more probable elements and longer
representations to less probable elements. Conversely, any compression
scheme can be interpreted as a statement of belief regarding the
various probabilties of elements in the data space. The best
compression will be that which faithfully assigns representations of
the appropriate lengths under the true distribution of the data; this
is formalized by the well-known Kraft inequality. If we associate with
each element a complexity, measured according to its representation
length in a given compression scheme, then we are claiming that more
probable elements are less complex.










\end{document}

