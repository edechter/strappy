\documentclass{article}

\usepackage{ijcai13}
\usepackage{times}
\usepackage{amsmath} 
\usepackage{latexsym} 
\usepackage{ dsfont }
\usepackage{tikz}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usetikzlibrary{arrows,automata}

\DeclareMathOperator*{\argmax}{arg\,max} 

\title{The Exploration-Compression Algorithm: how to learn programs by discovering useful subroutines.}
\author{Eyal Dechter \\
MIT\\
USA \\
edechter@mit.edu
\And
Josh Tenenbaum \\
MIT\\
USA \\
jbt@mit.edu
\And 
Ryan Adams \\
MIT\\
USA \\
rpa@seas.harvard.edu}

\begin{document}

\maketitle

\begin{abstract}
  Every programmer knows that the abstractions provided by a good
  function library are not merely a convenience: they usefully
  constrain the space of implementations and guide the choice of
  programming patterns. Without these abstractions, whether explicit
  or implicit, the designer would wander in a sea of
  detail. Similarly, the learner who wishes to discover the theories
  and concepts that explain the world must use a set of abstractions
  to guide their development. But how do those abstractions arise?
  Here, we show how alternating between a search over program space
  and a compression of discovered programs allows a learner to build
  up abstractions that make the learner's search progressively more
  effective.
\end{abstract}

\section{Introduction.}

\section{Related work.}

\section{A multi-task program induction problem.}
Our goal is to solve a general multi-task learning problem: we are
given set of \emph{tasks} $T=\{t_k\}_{k=1}^K$ where each task is a
function $t_i : \mathcal{L} \rightarrow \mathds{R} > 0$ which returns
a reward value for every element in the solution representation
language $\mathcal{L}$. Our goal is to find a set of solutions
$X = \{x_k\}_{k=1}^k \subset \mathcal{L}$ such that 

\[
X = \argmax_{x_1, \dots, x_K \in L} \sum_k t_k(x_k).
\]

\section{A compression scheme expresses a belief about the data.}
There is a deep connection between a distribution over data and the
optimally compressive code for that data. In fact, if we know the
probability distribution for a set of observations, we can obtain an
optimally compressive encoding of those observations via arithmetic
coding. Conversely, the encoding generated by a good compression
scheme can be interpreted as an approximation of the data
distribution. 

Take, for example, the Nevill-Manning algorithm for grammar based
compression~\cite{nevill1997identifying}. This algorithm looks for
hierarchical structure in the data and represents that hierarchical
structure as a grammar. The output of the algorithm can be seen as a
grammar and a corresponding parse of the input data. Given that parse,
we can estimate the maximum likelihood probabilistic context-free
grammar for that data~\cite{johnson1998pcfg}. 

\section{The E-C Algorithm.}

In this section, we describe the E-C algorithm and discuss its uses,
applicability, and limitations.

As its name suggests, each iteration of the E-C algorithm consists of
an exploration step and a compression step. In the exploration step,
we enumerate the $N$ most probable hypotheses given our currenst
distribution $D$ over hypotheses. We then assign to each task $t_i$
the hypotheses that match it. We say that a task is \emph{hit} if
there is a least one hypothesis that matches it. In the compression
step, our goal is to assign a single hypothesis $h_i^*$ to each task
$t_i$ such that that resulting set of hypotheses $\{t_i\}$ is as
compressible as possible. The compression this process induces is used
to update distribution $D$.

  \begin{algorithm}
    \SetAlgoLined \KwData{Tasks $T=\{t_1, \dots, t_K\}$,
      distribution $\mathcal{D}_j$ over expressions.}  

    $\text{types} \leftarrow \cup_k \, type(t_k)$. \\
    For $\tau \in \text{ types}$ , $H_N(\tau) \leftarrow$ enumerate
    $N$ most probable elements in $\mathcal{D}_t$ with type $\tau$.\\

    For each \emph{hit} $s_i$ in 
    $T$, $v_{s_i} \leftarrow \{h \in D_j | s_i(h) == True \}$.\\

    $S = \{(s_i, h^*_{s_i})\} \leftarrow$ assignment of elements to tasks s.t. 
    $S$ is most compressible. \\

    $D_{j+1} \leftarrow$ derive new distribution from compression of $S$. \\

    Repeat. 

    \caption{E-C}
  \end{algorithm}



\section{Our implementation.}

\subsection{Representing polymorphically typed programs as binary trees.}

Following~\cite{liang10programs} and~\cite{Briggs:2008}, we use a
combinatory logic -- a variable-free subset of the polymorphic
simply-typed lambda calculus lambda calculus~\cite{Pierce_2002} -- as
our program representation language. In short, the combinatory logic
introduces a \emph{basis} of several primitive \emph{combinators} such
that any function in the lambda calculus can be alternatively written
as applications of the basis combinators. The basis combinators can
themselves been expressed in the lambda calculus. The lambda calculus
has two basic operation -- application and abstraction -- but in using
the combinatory logic we sequester uses of the abstraction operation
inside the combinators, and thus have a representation language that
is effectively abstraction- and, thus, variable-free. 

This is very convenient for program synthesis: since every expression
is the application of one expression to another -- with this recursion
bottoming out at the primitive combinators -- each program is a binary
tree. Most importantly, any subtree is itself a well-formed
expression; this is not the case in the lambda calculus, since
abstraction introduces long range dependencies between the $lamdba$-
operator and the variables to which that operator refers. In the
lambda calculus, then, a subtree might have free variables, not bound
to any enclosing $lambda$. 

\subsection{Enumerating the most likely programs.}
The simplicity of our program representation suggests a simple
recursive recipe for generating all programs of a given type $\tau$ up
to some maximal depth $d$: such a program is either one of the atomic
programs with type in $\tau$, or it is pairing of a left child program
and a right child program with appropriate types each of which comes
from corresponding sets of programs with depth up to $d-1$. The left
child is any program with type $\sigma' \leftarrow \tau'$ which is a
subset of $\sigma \leftarrow \tau$, where $\sigma$ is a free type
variable. To match, the right child must have type $\sigma'$. The
resulting program will have type $\tau'$ which is a subset of $\tau$.

  \begin{algorithm}
    \SetAlgoLined 
    
    \KwData{Library of primitive/type pairs: $\mathcal{L} = \{(f_1, \tau_1), \dots,
        (f_m, \tau_m)\}$. Target type $\tau^*$. Max depth $d$.}

    Choose $F$ from $\text{ ENUM}(\mathcal{L}, \tau^*, 0)$, or:
    
    Choose $L$ from $\text{ ENUM}( \mathcal{L}, \sigma \rightarrow
    \tau^*, d-1)$.

    Choose $R$ from $\text{ ENUM}( \mathcal{L}, sourceType(L), d-1)$.

    Return $F = (L\,R)$ with type $targetType(L)$.
    
    \caption{ENUM}
  \end{algorithm}



\subsection{Finding the most compressive solutions.}

\subsection{Re-estimating the probabilistic grammar over programs.}

\section{Experiments.}
\subsection{Integers: a toy case study.}
\subsection{Symbolic regression.}
\subsection{Learning electric circuits.}
\begin{figure}[h]
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
                    , minimum size=.5cm, thick,main node/.style={circle,draw}]

  \node[main node] (S) {$\{\}$};
  \node[main node] (1) [below left of=S] {$\{a\}$};
  \node[main node] (2) [below right of=S] {$\{b\}$} ;
  \node[main node] (3) [below left of=1] {$\{a, a\}$};
  \node[main node] (4) [below right of=1] {$\{a, b\}$};
  \node[main node] (5) [below right of=2] {$\{b, b\}$};;

  \path
    (S) edge node [left] {a} (1)
        edge node [right] {b} (2)
    (1) edge node [left] {a} (3)
        edge node [right] {b} (4)
    (2) edge node [left] {a} (4)
        edge node [right] {b} (5)


\end{tikzpicture}
\end{figure}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{plain}
\bibliography{ijcai13}
\nocite{*}

\end{document}

