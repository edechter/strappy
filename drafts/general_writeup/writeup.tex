\documentclass{article}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath} 
\usepackage{algpseudocode}
\usepackage{ amssymb }
\usepackage{listings}
\usepackage{syntax}

\newcommand{\funtype}[2]{#1 \rightarrow #2}
\newcommand{\weightedrule}[3]{[#1 \mapsto #2 \, | \, #3]}
\newcommand{\typedexp}[2]{#1 : #2} 
\newcommand{\type}[1]{type(#1)}

\renewcommand{\grammarlabel}[2]{#1 #2}
\newcommand{\given}{\, | \,}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}


\lstloadlanguages{Haskell}
\lstnewenvironment{code}
    {\lstset{}%
      \csname lst@SetFirstLabel\endcsname}
    {\csname lst@SaveFirstLabel\endcsname}
    \lstset{
      basicstyle=\small\ttfamily,
      flexiblecolumns=false,
      basewidth={0.5em,0.45em},
      literate={+}{{$+$}}1 {/}{{$/$}}1 {*}{{$*$}}1 {=}{{$=$}}1
               {>}{{$>$}}1 {<}{{$<$}}1 {\\}{{$\lambda$}}1
               {\\\\}{{\char`\\\char`\\}}1
               {->}{{$\rightarrow$}}2 {>=}{{$\geq$}}2 {<-}{{$\leftarrow$}}2
               {<=}{{$\leq$}}2 {=>}{{$\Rightarrow$}}2 
               {\ .}{{$\circ$}}2 {\ .\ }{{$\circ$}}2
               {>>}{{>>}}2 {>>=}{{>>=}}2
               {|}{{$\mid$}}1               
    }

\DeclareMathOperator*{\argmax}{arg\,max} 

\begin{document}

\newcommand{\learner}{\emph{Learner }}
\newcommand{\evaluator}{\emph{Evaluator }}
\section{The problem}
In all but the simplest problems, problem solving agents suffer from vast
search spaces. Much of the research in AI, machine learning, and optimization
focuses on how to overcome this search problem. Local information can help find
better nearby solutions but make few gaurantees about global optima. Heuristic
search techniques make use of the structure of the search space and information
about the utility of partial solutions to hopefully minimize the numeber of
candidate solutions that need to be explored. Other frameworks attempt to
approximate the target search space with a more tractable, convex, surrogate
search space. 

But since the begining of AI, a central idea -- at center stage at times and
lurking in the shadows at others -- is that as problem solving agents solve
more problems they should learn to solve problems better.

Here, we present a unified perspective on learning to solve problems, grounding the
problem as one of Bayesian inference over a grammar that generates candidate
problem solutions. 


Types:

\begin{grammar}
<$t$> ::= $\tau$, $\sigma$  (type variables)
  \alt $a$, $b$ (concrete types e.g. real, bool, int, etc.)
  \alt $\funtype{t'}{t}$ (function type)
\end{grammar}


Expressions: 
\begin{grammar}
<$\typedexp{e}{t}$> ::= $\typedexp{c}{t'}$ s.t. $t' = mgu(t,
\type{c})$ (substitute a terminal tree that unifies with the requesting type)
  \alt ($\typedexp{e}{\funtype{t'}{t}} \;\;\; \typedexp{e}{t'}$)
\end{grammar}




First we will present the model over solutions in which our problem solving
agent will be doing inference. Second, we will present a hybrid inference and
problem solving algorithm. 

\section{The model}
Our goal in this section is to describe a generic probability distribution over
solutions spaces. Learning to search will be implemented by learning a
distribution over the solution space which describes the probability of a 
candidate solution being the solution for a given problem. 

Problems vary considerably in their solution spaces. Some are numeric and
continuous, others are integral and discrete, and still others consist of
structured data like trees. Some solutions have a finite number of elements, or
are bounded in some fashion. Others are infinite and unbounded. To present a
framework for general problem solving, we need a representation of a solution
space that subsumes all of these possibilities. 

As has long been noted by
Genetic Programming researchers, any solution space can be modeled as a set of
programs that generate space. As an example, if the solution space to a problem
is the set of all boolean circuits constructed from some set of gates, then the
corresponding space of programs is the set of programs that wire up boolean
circuits from those gates. 

How does this realization help us? It means that if we can formulate a single
model that places a distribution over solution distributions by describing a
distribution over program generating processes.

We will first describe a simplified version of the hierarchical model over programs that we will be using. We start with a base grammar $\mathcal{G}$ of the form 
\section{The model}
The elements of any solution space can be represented as programs. u
 \section{History and related literature}
Angluin 1988~\cite{angluin1988queries}: investigates the complexity of
learning concepts that are subsets of a hypothesis space via a number
of primitive queries such as membership, equivalence, subset, etc. \\

Valiant 1984~\cite{valiant1984theory}: original paper on Valiant's
computational learning theory.

Shapiro 1981~\cite{shapiro1981algorithm}: on the model inference
problem. Focused on learning programs or axioms from a set of positive
and negative examples. Explicitly related to Kuhnian philosophy of
science.

Inferring a program for computing a recursive function: 
Gold 65~\cite{gold1965limiting}, 
Gold 67~\cite{gold1967language},
Gold 78 ~\cite{gold1978complexity},
Blum  Blum 75~\cite{blum1975toward}

Inductive Logic Programming (ILP): ILP has a rich history and
literature, and in some sense its goals are quite close to ours. In
recent years, Genetic Programming style techniques have been applied
to ILP; see~\cite{divina2006evolutionary}.

\section{Type Theory}
In using type theory to facilitate program induction we will mostly be
concerned with using the types of subprograms to determine how
subprograms can be glued together. This focus is nicely stated by Girard~\cite{girard1989proofs}:

\begin{quote}
At a more general level, abstracting away from any peculiar syntactic
choice, one should see a type as an instruction for \emph{plugging}
things together. let us imagine that we program with \emph{modules},
\emph{i.e.} closed units, which we can plug together. A module is
absolutely closed, we have no right to open it. We just have the
ability to use it or not, and to choose the manner of use
(plugging). The type of module is a of course completley determined by
all the possibling \emph{pluggings} it allows without crashing. In
parcticular, one can always susbtitute a module with another of the
same type, in the vent of a breakdown, or for the purpose of
optimisation.
\end{quote}

\section{Learnable Curriculum}
We would like to formally characterize when a program can be learned
that represents elements in a given dataset. In general such programs
always exist; we can simply iterate over all recursive functions until
we find a set that evaluates to each element in the dataset. However,
such a search will usually take a prohibitively long time, so we wish
to restrict our search to short programs.  Thus, we say that a datum
is learnable if a sufficiently short program exists that evaluates to
it. This is simply the statement that the Kolmogorov complexity
$K_U(x)$ of a datum $x$ (relative to our universal computer $U$) is
less than some fixed number of bits $r$. 

Moreover, we should be able to use previously learned programs to help
us search for new programs. This phenomenon, in which a previously
learned datum is used to represent a later learned datum, we will term
\emph{bootstrapping}. We say that $x$ \emph{bootstraps} $y$ if the
conditional Kolmogorov complexity $K(y | p_x) < r$ when $U(p_x) = x$ and
$r$ is again our resource bound. 

Let $D= \{d_1, \dots, d_N\}$ be a \emph{dataset}. A \emph{curriculum}
$C$ for $D$ is an ordered tuple $C = (c_1, \dots, c_M)$ of elements in
$D$. Not all data $d_i$ must appear in $C$, and each $d_i$ can appear
more than once.

\section{Model of Computation: Combinatory Logic}
The representational language and related computational model used to
implement our learner is a typed Combinatory Logic
(CL)~\cite{hindley1972introduction}. CL is symbolic model of
computation that is similar to the more popular $\lambda$-calculus,
but, for our purposes, has one very significant difference: it does
not contain variables. $\lambda$-calculus has three basic syntactic
features -- variables, applications, and abstractions -- that
correspond appealingly with intuitions from mathematics and functional
programming. But besides the semantic difficulties that variables
cause (documented elsewhere), variables additionally complicate the
modularity and compositionality of expressions written in
$\lambda$-calculus. Program induction requires the efficient search
over expressions in the chosen model of computation; being able to
treat most subexpressions of well-formed expressions as themselves
well-formed expressions makes things easier. This was first noticed by
Briggs and O'Neill in using combinators for genetic
programming~\cite{briggs2006functional}. 

Example: Consider the function $f = \lambda y. \lambda x. +\,
(*\, x\, x) (*\, x\, x)$ (note that all functions we are considering
will be \emph{curried}; that is, all functions will be functions of
only one argument, and functions of multiple arguments will be
represented by higher-order functions of a single argument such that
the application of the function to an argument returns a function
whose argument is the next argument in the original series of
arguments.)



\section{Algorithm 0}
In order to motivate our approach, we present the following problem:
suppose we have a probability model $P(D, E, \mathcal{L}) = P(D|E)
P(E|L) P(L)$ where $L$ represents the parameters of a latent model,
$E_i$ are drawn $i.i.d$ from $P(\dot | L)$ and $D_i$ are drawn $i.i.d$
from $P(\dot|E_i)$. The $E_i$ are hidden, the $D_i$ observed. Suppose
we are able to 1) draw sampled of $E_i$ given $L$ and 1) we can
enumerate $E_i$ in order of decreasing probability under $L$. Our goal
is to determine the most likely $L$ given the data $D$. A rejection
sampling algorithm approach might involve sampling the $E_i$ from the
current value of $L$, accepting $E_i$ if  To solve this problem, we propose
Algorithm~\ref{alg:short}: at each iteration of our algorithm, we
choose

\begin{minipage}{.7\linewidth}
  \begin{algorithm}[H]
    \SetAlgoLined 
    \KwData{$\mathcal{D}=(D_1, \dots, D_M)$,
      $\mathcal{L}^{t-1}$ , $0 \geq K \leq 1$} 
    \KwResult{updated library $\mathcal{L}^t$}
    $E \leftarrow \{ E_i : p(E_i\, |\, \mathcal{L}^{t-1}) > K\}$\;
    $S \leftarrow \{ h \in H \cap D \}$\;
    $\mathcal{L}^t \leftarrow \argmax_L p(L \, | \, S)$
    \caption{Algorithm 0}
    \label{alg:short}
  \end{algorithm}
\end{minipage}


\section{Algorithm}
In this section, I describe the program learning algorithm. The
algorithm is parameterized by the following parameters: 

\begin{itemize}
\item depth $d$: this bounds the search over expressions at each
  iterations to expressions of depth $\leq d$;
\item log likelihood search bound $l$: this bounds the search over
  expressions at each iteration to expressions whose log likelihood is
  $\geq l$;
\item library prior $\mathcal{L}^0$: this data structure contains the
  primitive combinators and associated pseudo-counts that represent
  the background knowledge of the algorithm. This should also include
  any necessary base combinators (e.g. S, K, I, etc.).
\end{itemize}

The inputs to the algorithm are:
\begin{itemize}
\item a dataset $\mathcal{D} = (D_1, \dots, D_M)$, each of which is an
  expression in the language;
\item a evaluator $\mathcal{E}$ which is a function taking a datum
  $D_i$ and an associated expression $\phi_i$ and returns a reward
  signal $\mathcal{E}(D_i, \phi_i)$.
\end{itemize}

\begin{minipage}{.7\linewidth}
  \begin{algorithm}[H]
    \SetAlgoLined 
    \SetKwFunction{EnumerateCombinators}{EnumerateCombinators}
    \SetKwFunction{IsSuccessful}{IsSuccessful}
    \SetKwFunction{filter}{filter}
    \SetKwFunction{FindBestNewCombinator}{FindBestNewCombinator}
    \SetKwFunction{ReEstimateLibrary}{ReEstimateLibrary}
    \KwData{$\mathcal{D}=(D_1, \dots, D_M)$,
      $\mathcal{E}$, $\mathcal{L}^{t-1}$, $d$, $l$} 
    \KwResult{updated library $\mathcal{L}^t$}
    $E \leftarrow $ \EnumerateCombinators{$d$, $l$, $\mathcal{L}^{t-1}$}\;
    $E' \leftarrow $ 
      \filter{$\lambda e.$ \IsSuccessful{$\mathcal{E}$, $\mathcal{D}$, $e$}, $E$}\;
    $C \leftarrow$ \FindBestNewCombinator{$E'$}\;
    $\mathcal{L}^t \leftarrow$ \ReEstimateLibrary{$\mathcal{L}^{t-1}$, $C$, $E'$}\;
    \caption{OneStep}
  \end{algorithm}
\end{minipage}


\section{Language Implementation}
I've found various resources to guide implementation of typed
languages.  A simple guide to the implementation of dependently typed
programming languages is found in Loh et al.~\cite{lowsimply}.

\section{Holes}
In real-world applications, many of the useful degrees of freedom are
continuous, and it would be useful to have parameters that could be
fit in a systematic way. Doing this requires a small extension to the
framework presented so far: we add a special type of construct as a
primitive of the language. We call these holes. Each hole is a
placeholder for values of a specific type, within a specific
domain. For example, a continuous hole is bounded within a continuous
integer and has the type of a real value.

A combinator with no holes is filled.


\section{Genetic Programming}

The most directly related work to ours is that on Genetic Programming.
Therefore, we need to declare the differences between our work and genetic
programming in as clear a way as possible. This will consist of two steps: 1)
we need to review the literature and understand what its achievements are; 2)
we need to implement a GP system and show how our system differs from a GP
system in practice. 

An implementation of a symbolic regression genetic programming system is
TinyGP, Java source code for which can be found in Appendix B
of~\cite{poli08:fieldguide}. 

There are five basic things that need to be specified in order to create a GP
algorithm. We need to determine the terminal set, the function set, the fitness
function, the GP parameter values, and the terminal solution designation. The
terminal set consists of the external inputs (i.e.  variables of the form x, y,
etc.), functions with no arguments (e.g., rand and other functions with side
effects), and constants. 

The function set is a set of functions and their arities. Function sets must
satisfy \emph{closure}, which consists of \emph{type consistency} and
\emph{evaluation safety}. Type consistency means that all functions return
values of the same type. Evaluation safety means that functions do not throw
errors. 

The fitness function is the objective function. 

The GP parameters are various: population size, probabilities of performing
genetic operations (crossover and mutation), selection mechanism, etc. 

Finally, we need to specify termination condition. These include the number of
runs or other predicates based on which the program should terminate.

There are several papers by O'Reilly that analyze GP that are worth looking at:
\cite{OReilly:thesis} \cite{OReilly:1995:tabbGP}

\subsection{Modulearity and Hierarchy in GP} 

Koza's Automatically defined functions (ADFs) are the most popular way of
evolving reusable components. In automatically defined functions, we explicitly
add auxiliary functional genetic material that the main function can call.
Typically, the ADFS need different terminal and function sets (for example,
ADFs need argument terminals). In this scheme, each function only has access to
its own ADFs. See Olsson's ADATE system where subfunctions as anonymous lambda
expressions are extracted~\cite{DBLP:conf/eurogp/Olsson9}.

\subsection{Software}
from page 148 of \cite{poli08:fieldguide}:
\begin{itemize}
\item Lil-GP (Punch and Zongker, 1998)
\item ECJ (Luke, Panait, Balan, Paus, Skolicki, Popvici, Harrison, Bassett,
Hubley, and Chircop, 2000-2007)
\item Open Beagle (Gahge and Parizeau, 2002)
\item GPC++ (Fraser and Weinbrenner)
\item commercial: Discipulus (RML Technologies)
\item review: Foster, 2001 
\end{itemize}

\bibliographystyle{plain}
\bibliography{writeup}

\section{Density estimation with program planning} 
Density estimation -- an unsupervised learning task whose importance is
increasing in modern machine learning -- is the process of learning the
probability density function associated with some set of data. The utility of
doing this comes from the fact that a low dimensional representation of the
learned density function is useful in other tasks.

Suppose, for example, that we wanted to learn the distribution over real-world
images from a dataset of images that we download from the web. The space of
possible images is extremely large, but space of images that are probable in
the real world is vanishingly small within the space of all possible images. To
learn the distribution of images means that a new real world image will be
assigned a relatively high likelihood under our learned distribution and noise,
for example, would be assigned a low likelihood.




\end{document}

